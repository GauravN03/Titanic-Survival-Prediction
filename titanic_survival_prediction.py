# -*- coding: utf-8 -*-
"""Titanic Survival Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Co6Uq-T_9l-nD0CfnmAws5PN-mx_-B_
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder

# loading the data
df=pd.read_csv('/content/tested.csv')
df.head()

#checking the data types of columns
df.info()

# checking null values
df.isnull().sum()

# Statistical summary
df.describe()

# Handle missing values
imputer = SimpleImputer(strategy='mean')  # Or 'median', 'most_frequent'
df['Age'] = imputer.fit_transform(df[['Age']])

# Encode categorical features
encoder = OneHotEncoder(sparse_output=False)  # sparse=False for compatibility
encoded_features = encoder.fit_transform(df[['Sex', 'Embarked']])  # Select categorical columns
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(['Sex', 'Embarked']))
df = pd.concat([df, encoded_df], axis=1)
df.drop(['Sex', 'Embarked'], axis=1, inplace=True)  # Remove original columns

df

df.isna().sum()

df.info()

df['Age'] = pd.to_numeric(df['Age'], errors='coerce')

df.drop(columns=['Title'], axis=1, inplace=True)

df



df['TicketPrefix'] = df['Ticket'].str.split().str[0]
# Extract the first part of the ticket string as the prefix

# Encode ticket prefixes using Label Encoding
le = LabelEncoder()
df['TicketPrefix_Encoded'] = le.fit_transform(df['TicketPrefix'])
df['TicketLength'] = df['Ticket'].str.len()
df.drop(columns=['Ticket'], axis=1, inplace=True)

df.info()

df

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df['TicketPrefix_Encoded'] = le.fit_transform(df['TicketPrefix'])
df.drop(columns=['TicketPrefix'], axis=1, inplace=True)  # Remove original column

X=df.drop(columns=['Survived'])
y=df['Survived']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle missing values after all feature engineering and encoding steps are performed
imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

sc=StandardScaler()
X_train_sc=sc.fit_transform(X_train)
X_test_sc=sc.transform(X_test)

model_log=LogisticRegression()
model_log.fit(X_train_sc,y_train)

training_accuracy = model_log.score(X_train_sc, y_train)
print('Training Accuracy:', training_accuracy)
testing_accuracy = accuracy_score(y_test, model_log.predict(X_test_sc))
print('Testing Accuracy:',testing_accuracy)
overall_accuracy = accuracy_score(y_test, model_log.predict(X_test_sc))
print('Overall Accuracy:', overall_accuracy)

cof_mat=confusion_matrix(y_test,model_log.predict(X_test_sc))
print(cof_mat)
conf_display=ConfusionMatrixDisplay(confusion_matrix=cof_mat,display_labels=model_log.classes_)
conf_display.plot(cmap='Blues')
plt.show()

y_pred_proba = model_log.predict_proba(X_test_sc)
print(y_pred_proba)

proba_df = pd.DataFrame(y_pred_proba, columns=['Probability_Class_0', 'Probability_Class_1'])
proba_df = proba_df.round(2)
proba_df['Actual_Value'] = y_test.values

# Get predicted values
y_pred = model_log.predict(X_test_sc)

# Add the predicted values to the DataFrame
proba_df['Predicted_Value'] = y_pred


# Display the DataFrame
print(proba_df)